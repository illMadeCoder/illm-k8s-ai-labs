{
  "name": "db-baseline-fsync-2rst6",
  "namespace": "experiments",
  "description": "Baseline: naive fsync-per-write store on GKE PD-SSD — measures the floor cost of durable 4-byte sequential writes with no WAL, no batching, no optimization",
  "createdAt": "2026-02-18T00:52:26Z",
  "completedAt": "2026-02-18T01:29:36.127206859Z",
  "durationSeconds": 2230.127206859,
  "phase": "Complete",
  "tags": [
    "baseline",
    "storage",
    "database"
  ],
  "hypothesis": {
    "claim": "A naive fsync-per-write store on GKE PD-SSD achieves <5ms p99 write latency for sequential 4-byte writes, establishing the floor cost of durable persistence",
    "questions": [
      "What is the true fsync cost on GKE PD-SSD for single-row durable writes?",
      "How does read latency compare to write latency when reads are served from page cache?",
      "What sustained write throughput is achievable with fsync-per-write serialization?"
    ],
    "focus": [
      "fsync latency distribution on cloud block storage",
      "write throughput ceiling under serialized I/O",
      "page cache hit rate for sequential reads"
    ]
  },
  "analyzerConfig": {
    "sections": [
      "abstract",
      "targetAnalysis",
      "performanceAnalysis",
      "metricInsights",
      "finopsAnalysis",
      "body",
      "feedback",
      "architectureDiagram"
    ]
  },
  "targets": [
    {
      "name": "app",
      "clusterName": "db-baseline-fsync-2rst6-app",
      "clusterType": "gke",
      "machineType": "e2-standard-4",
      "nodeCount": 1,
      "components": [
        "ConfigMap/alloy",
        "ConfigMap/kube-prometheus-stack-alertmanager-overview",
        "ConfigMap/kube-prometheus-stack-apiserver",
        "ConfigMap/kube-prometheus-stack-cluster-total",
        "ConfigMap/kube-prometheus-stack-controller-manager",
        "ConfigMap/kube-prometheus-stack-etcd",
        "ConfigMap/kube-prometheus-stack-grafana",
        "ConfigMap/kube-prometheus-stack-grafana-config-dashboards",
        "ConfigMap/kube-prometheus-stack-grafana-datasource",
        "ConfigMap/kube-prometheus-stack-grafana-overview",
        "ConfigMap/kube-prometheus-stack-k8s-coredns",
        "ConfigMap/kube-prometheus-stack-k8s-resources-cluster",
        "ConfigMap/kube-prometheus-stack-k8s-resources-multicluster",
        "ConfigMap/kube-prometheus-stack-k8s-resources-namespace",
        "ConfigMap/kube-prometheus-stack-k8s-resources-node",
        "ConfigMap/kube-prometheus-stack-k8s-resources-pod",
        "ConfigMap/kube-prometheus-stack-k8s-resources-workload",
        "ConfigMap/kube-prometheus-stack-k8s-resources-workloads-namespace",
        "ConfigMap/kube-prometheus-stack-kubelet",
        "ConfigMap/kube-prometheus-stack-namespace-by-pod",
        "ConfigMap/kube-prometheus-stack-namespace-by-workload",
        "ConfigMap/kube-prometheus-stack-node-cluster-rsrc-use",
        "ConfigMap/kube-prometheus-stack-node-rsrc-use",
        "ConfigMap/kube-prometheus-stack-nodes",
        "ConfigMap/kube-prometheus-stack-nodes-aix",
        "ConfigMap/kube-prometheus-stack-nodes-darwin",
        "ConfigMap/kube-prometheus-stack-persistentvolumesusage",
        "ConfigMap/kube-prometheus-stack-pod-total",
        "ConfigMap/kube-prometheus-stack-prometheus",
        "ConfigMap/kube-prometheus-stack-proxy",
        "ConfigMap/kube-prometheus-stack-scheduler",
        "ConfigMap/kube-prometheus-stack-workload-total",
        "Namespace/observability",
        "Secret/alertmanager-kube-prometheus-stack-alertmanager",
        "Secret/kube-prometheus-stack-grafana",
        "Service/alloy",
        "Service/kube-prometheus-stack-alertmanager",
        "Service/kube-prometheus-stack-grafana",
        "Service/kube-prometheus-stack-kube-state-metrics",
        "Service/kube-prometheus-stack-operator",
        "Service/kube-prometheus-stack-prometheus",
        "Service/kube-prometheus-stack-prometheus-node-exporter",
        "Service/naive-db",
        "Service/kube-prometheus-stack-coredns",
        "Service/kube-prometheus-stack-kube-controller-manager",
        "Service/kube-prometheus-stack-kube-etcd",
        "Service/kube-prometheus-stack-kube-proxy",
        "Service/kube-prometheus-stack-kube-scheduler",
        "Service/vm-hub",
        "ServiceAccount/alloy",
        "ServiceAccount/kube-prometheus-stack-admission",
        "ServiceAccount/kube-prometheus-stack-alertmanager",
        "ServiceAccount/kube-prometheus-stack-grafana",
        "ServiceAccount/kube-prometheus-stack-kube-state-metrics",
        "ServiceAccount/kube-prometheus-stack-operator",
        "ServiceAccount/kube-prometheus-stack-prometheus",
        "ServiceAccount/kube-prometheus-stack-prometheus-node-exporter",
        "MutatingWebhookConfiguration/kube-prometheus-stack-admission",
        "ValidatingWebhookConfiguration/kube-prometheus-stack-admission",
        "CustomResourceDefinition/alertmanagerconfigs.monitoring.coreos.com",
        "CustomResourceDefinition/alertmanagers.monitoring.coreos.com",
        "CustomResourceDefinition/podlogs.monitoring.grafana.com",
        "CustomResourceDefinition/podmonitors.monitoring.coreos.com",
        "CustomResourceDefinition/probes.monitoring.coreos.com",
        "CustomResourceDefinition/prometheusagents.monitoring.coreos.com",
        "CustomResourceDefinition/prometheuses.monitoring.coreos.com",
        "CustomResourceDefinition/prometheusrules.monitoring.coreos.com",
        "CustomResourceDefinition/scrapeconfigs.monitoring.coreos.com",
        "CustomResourceDefinition/servicemonitors.monitoring.coreos.com",
        "CustomResourceDefinition/thanosrulers.monitoring.coreos.com",
        "DaemonSet/alloy",
        "DaemonSet/kube-prometheus-stack-prometheus-node-exporter",
        "Deployment/kube-prometheus-stack-grafana",
        "Deployment/kube-prometheus-stack-kube-state-metrics",
        "Deployment/kube-prometheus-stack-operator",
        "StatefulSet/naive-db",
        "Job/kube-prometheus-stack-admission-create",
        "Job/naive-db-loadgen",
        "Alertmanager/kube-prometheus-stack-alertmanager",
        "Prometheus/kube-prometheus-stack-prometheus",
        "PrometheusRule/kube-prometheus-stack-alertmanager.rules",
        "PrometheusRule/kube-prometheus-stack-config-reloaders",
        "PrometheusRule/kube-prometheus-stack-etcd",
        "PrometheusRule/kube-prometheus-stack-general.rules",
        "PrometheusRule/kube-prometheus-stack-k8s.rules.container-cpu-usage-seconds-tot",
        "PrometheusRule/kube-prometheus-stack-k8s.rules.container-memory-cache",
        "PrometheusRule/kube-prometheus-stack-k8s.rules.container-memory-rss",
        "PrometheusRule/kube-prometheus-stack-k8s.rules.container-memory-swap",
        "PrometheusRule/kube-prometheus-stack-k8s.rules.container-memory-working-set-by",
        "PrometheusRule/kube-prometheus-stack-k8s.rules.container-resource",
        "PrometheusRule/kube-prometheus-stack-k8s.rules.pod-owner",
        "PrometheusRule/kube-prometheus-stack-kube-apiserver-availability.rules",
        "PrometheusRule/kube-prometheus-stack-kube-apiserver-burnrate.rules",
        "PrometheusRule/kube-prometheus-stack-kube-apiserver-histogram.rules",
        "PrometheusRule/kube-prometheus-stack-kube-apiserver-slos",
        "PrometheusRule/kube-prometheus-stack-kube-prometheus-general.rules",
        "PrometheusRule/kube-prometheus-stack-kube-prometheus-node-recording.rules",
        "PrometheusRule/kube-prometheus-stack-kube-scheduler.rules",
        "PrometheusRule/kube-prometheus-stack-kube-state-metrics",
        "PrometheusRule/kube-prometheus-stack-kubelet.rules",
        "PrometheusRule/kube-prometheus-stack-kubernetes-apps",
        "PrometheusRule/kube-prometheus-stack-kubernetes-resources",
        "PrometheusRule/kube-prometheus-stack-kubernetes-storage",
        "PrometheusRule/kube-prometheus-stack-kubernetes-system",
        "PrometheusRule/kube-prometheus-stack-kubernetes-system-apiserver",
        "PrometheusRule/kube-prometheus-stack-kubernetes-system-controller-manager",
        "PrometheusRule/kube-prometheus-stack-kubernetes-system-kube-proxy",
        "PrometheusRule/kube-prometheus-stack-kubernetes-system-kubelet",
        "PrometheusRule/kube-prometheus-stack-kubernetes-system-scheduler",
        "PrometheusRule/kube-prometheus-stack-node-exporter",
        "PrometheusRule/kube-prometheus-stack-node-exporter.rules",
        "PrometheusRule/kube-prometheus-stack-node-network",
        "PrometheusRule/kube-prometheus-stack-node.rules",
        "PrometheusRule/kube-prometheus-stack-prometheus",
        "PrometheusRule/kube-prometheus-stack-prometheus-operator",
        "ServiceMonitor/kube-prometheus-stack-alertmanager",
        "ServiceMonitor/kube-prometheus-stack-apiserver",
        "ServiceMonitor/kube-prometheus-stack-coredns",
        "ServiceMonitor/kube-prometheus-stack-grafana",
        "ServiceMonitor/kube-prometheus-stack-kube-controller-manager",
        "ServiceMonitor/kube-prometheus-stack-kube-etcd",
        "ServiceMonitor/kube-prometheus-stack-kube-proxy",
        "ServiceMonitor/kube-prometheus-stack-kube-scheduler",
        "ServiceMonitor/kube-prometheus-stack-kube-state-metrics",
        "ServiceMonitor/kube-prometheus-stack-kubelet",
        "ServiceMonitor/kube-prometheus-stack-operator",
        "ServiceMonitor/kube-prometheus-stack-prometheus",
        "ServiceMonitor/kube-prometheus-stack-prometheus-node-exporter",
        "ServiceMonitor/naive-db",
        "ClusterRole/alloy",
        "ClusterRole/kube-prometheus-stack-admission",
        "ClusterRole/kube-prometheus-stack-grafana-clusterrole",
        "ClusterRole/kube-prometheus-stack-kube-state-metrics",
        "ClusterRole/kube-prometheus-stack-operator",
        "ClusterRole/kube-prometheus-stack-prometheus",
        "ClusterRoleBinding/alloy",
        "ClusterRoleBinding/kube-prometheus-stack-admission",
        "ClusterRoleBinding/kube-prometheus-stack-grafana-clusterrolebinding",
        "ClusterRoleBinding/kube-prometheus-stack-kube-state-metrics",
        "ClusterRoleBinding/kube-prometheus-stack-operator",
        "ClusterRoleBinding/kube-prometheus-stack-prometheus",
        "Role/kube-prometheus-stack-admission",
        "Role/kube-prometheus-stack-grafana",
        "RoleBinding/kube-prometheus-stack-admission",
        "RoleBinding/kube-prometheus-stack-grafana"
      ]
    }
  ],
  "workflow": {
    "name": "db-baseline-fsync-2rst6-validation",
    "template": "db-baseline-fsync-validation",
    "phase": "Succeeded",
    "startedAt": "2026-02-18T01:03:48Z",
    "finishedAt": "2026-02-18T01:29:26Z"
  },
  "metrics": {
    "collectedAt": "2026-02-18T01:29:37.118980644Z",
    "source": "target:db-baseline-fsync-2rst6/kube-prometheus-stack-prometheus",
    "timeRange": {
      "start": "2026-02-18T00:52:26Z",
      "end": "2026-02-18T01:29:37.118367201Z",
      "duration": "37m11.118367201s",
      "stepSeconds": 60
    },
    "queries": {
      "diag_naivedb_any": {
        "query": "count({__name__=~\"naivedb.*\"})",
        "type": "instant",
        "unit": "count",
        "description": "Diagnostic: any naivedb metrics (no namespace filter)"
      },
      "diag_naivedb_up": {
        "query": "up{job=\"naive-db\"}",
        "type": "instant",
        "unit": "count",
        "description": "Diagnostic: naive-db scrape target status",
        "data": [
          {
            "labels": {
              "__name__": "up",
              "container": "naive-db",
              "endpoint": "http",
              "instance": "10.1.0.18:8080",
              "job": "naive-db",
              "namespace": "db-baseline-fsync-2rst6",
              "pod": "naive-db-0",
              "service": "naive-db"
            },
            "timestamp": "2026-02-18T01:29:37Z",
            "value": 0
          }
        ]
      },
      "diag_rows_nofilter": {
        "query": "naivedb_rows_total",
        "type": "instant",
        "unit": "rows",
        "description": "Diagnostic: total rows without namespace filter"
      },
      "diag_up_targets": {
        "query": "count(up)",
        "type": "instant",
        "unit": "count",
        "description": "Diagnostic: total scrape targets in Prometheus",
        "data": [
          {
            "timestamp": "2026-02-18T01:29:37Z",
            "value": 14
          }
        ]
      },
      "fsync_latency_over_time": {
        "query": "histogram_quantile(0.99, sum(rate(naivedb_fsync_duration_seconds_bucket{namespace=~\"db-baseline-fsync-2rst6\"}[1m])) by (le))",
        "type": "range",
        "unit": "seconds",
        "description": "P99 fsync latency over time"
      },
      "fsync_p50_latency": {
        "query": "histogram_quantile(0.5, sum(rate(naivedb_fsync_duration_seconds_bucket{namespace=~\"db-baseline-fsync-2rst6\"}[37m11s])) by (le))",
        "type": "instant",
        "unit": "seconds",
        "description": "P50 fsync-only latency"
      },
      "fsync_p99_latency": {
        "query": "histogram_quantile(0.99, sum(rate(naivedb_fsync_duration_seconds_bucket{namespace=~\"db-baseline-fsync-2rst6\"}[37m11s])) by (le))",
        "type": "instant",
        "unit": "seconds",
        "description": "P99 fsync-only latency"
      },
      "read_p99_latency": {
        "query": "histogram_quantile(0.99, sum(rate(naivedb_read_duration_seconds_bucket{namespace=~\"db-baseline-fsync-2rst6\"}[37m11s])) by (le))",
        "type": "instant",
        "unit": "seconds",
        "description": "P99 read latency (page cache)"
      },
      "read_throughput": {
        "query": "sum(rate(naivedb_operations_total{op=\"read\", namespace=~\"db-baseline-fsync-2rst6\"}[37m11s]))",
        "type": "instant",
        "unit": "ops/s",
        "description": "Read operations per second"
      },
      "total_rows": {
        "query": "naivedb_rows_total{namespace=~\"db-baseline-fsync-2rst6\"}",
        "type": "instant",
        "unit": "rows",
        "description": "Total rows written"
      },
      "write_latency_over_time": {
        "query": "histogram_quantile(0.99, sum(rate(naivedb_write_duration_seconds_bucket{namespace=~\"db-baseline-fsync-2rst6\"}[1m])) by (le))",
        "type": "range",
        "unit": "seconds",
        "description": "P99 write latency over time"
      },
      "write_p50_latency": {
        "query": "histogram_quantile(0.5, sum(rate(naivedb_write_duration_seconds_bucket{namespace=~\"db-baseline-fsync-2rst6\"}[37m11s])) by (le))",
        "type": "instant",
        "unit": "seconds",
        "description": "P50 write latency including fsync"
      },
      "write_p99_latency": {
        "query": "histogram_quantile(0.99, sum(rate(naivedb_write_duration_seconds_bucket{namespace=~\"db-baseline-fsync-2rst6\"}[37m11s])) by (le))",
        "type": "instant",
        "unit": "seconds",
        "description": "P99 write latency including fsync"
      },
      "write_throughput": {
        "query": "sum(rate(naivedb_operations_total{op=\"write\", namespace=~\"db-baseline-fsync-2rst6\"}[37m11s]))",
        "type": "instant",
        "unit": "ops/s",
        "description": "Write operations per second"
      }
    }
  },
  "costEstimate": {
    "totalUSD": 0.01660205809550589,
    "durationHours": 0.6194797796830556,
    "perTarget": {
      "app": 0.01660205809550589
    },
    "note": "Rough estimate based on on-demand GCE pricing; actual cost may differ."
  },
  "analysis": {
    "finopsAnalysis": {
      "overview": "This experiment ran a single e2-standard-4 GKE node for ~37 minutes at a total estimated cost of $0.017. The cost is minimal because the experiment is short-lived and uses only one node. The dominant expense is compute (the e2-standard-4 instance), with negligible storage costs for the PD-SSD volume holding 4-byte writes.",
      "costDrivers": [
        "Compute: e2-standard-4 instance ($0.134/hr on-demand) accounts for nearly all of the $0.017 experiment cost. The node runs the naive-db StatefulSet, a full kube-prometheus-stack (Prometheus, Grafana, Alertmanager, kube-state-metrics, node-exporter), and Alloy — a heavy observability footprint for a single-workload benchmark.",
        "Storage: PD-SSD provisioned for the naive-db StatefulSet. At 4 bytes per write with fsync-per-write serialization (~1000 ops/s), total data written is trivial (<50 MB over the experiment duration). PD-SSD costs $0.17/GB/month, so storage cost is effectively zero for this run. However, PD-SSD IOPS cost is the hidden driver — sustained fsync calls consume provisioned IOPS capacity."
      ],
      "projection": "For 24/7 production operation on non-preemptible nodes: (1) Compute: e2-standard-4 at $0.134/hr × 730 hrs/month = $97.82/month per node. A realistic production setup would run at least 3 nodes (app + monitoring separation + redundancy) = $293.46/month compute. (2) Storage: PD-SSD at $0.17/GB/month. A production-grade PD-SSD volume of 100 GB (for IOPS provisioning, since PD-SSD IOPS scale with volume size on GKE — 100 GB gives 3,000 read IOPS / 3,000 write IOPS) = $17.00/month. (3) Monitoring stack: Prometheus with 15-day retention on a separate PD-SSD (50 GB) = $8.50/month. (4) Network egress for metrics forwarding (Alloy/vm-hub): ~$1-5/month depending on volume. Total estimated monthly production cost: ~$320-$325/month. To reduce this, switch to e2-standard-2 if the workload fits (halving compute to ~$147/month for 3 nodes), or use committed use discounts (31% savings) bringing 3-node compute to ~$202/month and total to ~$228/month.",
      "optimizations": [
        "Right-size the monitoring stack: The experiment deploys a full kube-prometheus-stack with Grafana, Alertmanager, kube-state-metrics, node-exporter, and Alloy on the same single node as the workload. For a benchmark that produces <10 custom metrics, a lightweight Prometheus with remote-write (no Grafana/Alertmanager in-cluster) would free ~1.5 GB RAM and 0.5 vCPU, potentially allowing a downgrade to e2-standard-2 ($0.067/hr, 50% compute savings).",
        "Use committed use or spot instances for benchmarking: This experiment ran for only 37 minutes on on-demand pricing. For repeated benchmark runs, using spot/preemptible instances ($0.040/hr for e2-standard-4, ~70% savings) is safe since benchmark data is ephemeral. Estimated per-run cost drops from $0.017 to $0.005.",
        "PD-SSD volume sizing for IOPS: On GKE, PD-SSD IOPS scale linearly with volume size (30 IOPS/GB read, 30 IOPS/GB write). A 10 GB volume only provides 300 write IOPS — marginal for fsync-per-write at ~1000 ops/s. Either provision a larger volume (34+ GB for 1,000 write IOPS) or switch to Hyperdisk Extreme for decoupled IOPS provisioning to avoid throttling that inflates fsync latency.",
        "Cluster lifecycle automation: The experiment ran for 37 minutes but the cluster existed for ~37 minutes total (created at 00:52, completed at 01:29). Ensure cluster auto-deletion is configured to avoid paying for idle nodes if experiment cleanup fails."
      ]
    },
    "feedback": {
      "recommendations": [
        "Most naivedb metric queries returned no data (fsync latency, write latency, read latency, throughput, total rows all empty), and the scrape target shows up=0 at collection time. Investigate whether the naive-db pod crashed or was terminated before metrics were scraped — add a readiness gate or post-stop hook to ensure final metrics are captured before teardown.",
        "Add a Prometheus recording rule or a sidecar that periodically snapshots histogram buckets to a persistent volume, so even if the workload pod terminates before the final scrape, the latency distribution data is preserved.",
        "Increase the loadgen and naive-db overlap window with the Prometheus scrape interval — ensure at least 2-3 full scrape cycles (e.g., 60-90s at 30s scrape interval) occur after the loadgen completes but before the naive-db pod is terminated, so range queries have enough data points.",
        "Emit a summary metric (e.g., naivedb_summary_total_writes, naivedb_summary_p99_write_seconds) as a gauge at workload completion that persists the final computed values, providing a fallback when histogram rate queries return empty due to short-lived workloads."
      ],
      "experimentDesign": [
        "The experiment ran for ~37 minutes but produced no usable latency or throughput data — the primary failure is a lifecycle/observability gap. Redesign the workflow so the validation step queries Prometheus while the naive-db pod is still running and actively serving metrics (up=1), rather than after it has already terminated (up=0).",
        "Add explicit data-availability assertions to the workflow: before proceeding to the analysis phase, verify that key queries (write_p99_latency, fsync_p99_latency, total_rows) return non-empty results, and fail fast with a descriptive error if they don't. This prevents silent data-loss experiments from appearing as 'Succeeded'.",
        "Include node-level disk I/O metrics (node_disk_write_time_seconds_total, node_disk_writes_completed_total) from node-exporter as a cross-check — these persist independently of the application pod lifecycle and can validate that writes actually occurred even if application-level metrics are lost."
      ]
    },
    "body": {
      "blocks": [
        {
          "type": "text",
          "content": "This experiment attempted to measure the floor cost of durable persistence on GKE PD-SSD using a naive fsync-per-write store with 4-byte sequential writes. The hypothesis — that p99 write latency would land under 5ms — could not be validated because the naive-db scrape target was down (up=0) at metrics collection time, resulting in empty data for all latency and throughput queries."
        },
        {
          "type": "topic",
          "title": "Data Collection Failure",
          "blocks": [
            {
              "type": "text",
              "content": "The central issue of this experiment is not performance but observability: the naive-db pod was no longer scrapeable when Prometheus collected final metrics. This means every latency, throughput, and row-count query returned empty results."
            },
            {
              "type": "row",
              "blocks": [
                {
                  "type": "metric",
                  "key": "diag_naivedb_up",
                  "size": "small",
                  "insight": "Scrape target was down (value=0) at collection time — the pod likely terminated before the final scrape window"
                },
                {
                  "type": "metric",
                  "key": "diag_up_targets",
                  "size": "small",
                  "insight": "14 total scrape targets were active in Prometheus, confirming the monitoring stack itself was healthy"
                }
              ]
            },
            {
              "type": "metric",
              "key": "diag_naivedb_any",
              "size": "large",
              "insight": "Returns no data — no naivedb_* metrics exist in Prometheus at query time, confirming complete data loss"
            },
            {
              "type": "callout",
              "variant": "warning",
              "title": "All Performance Metrics Are Empty",
              "content": "The queries for write_p99_latency, fsync_p99_latency, read_p99_latency, write_throughput, read_throughput, and total_rows all returned no data. The experiment workflow phase shows 'Succeeded', masking the fact that no usable measurements were captured. The workflow needs data-availability assertions to fail fast when metrics are missing."
            },
            {
              "type": "table",
              "headers": [
                "Metric",
                "Expected",
                "Actual"
              ],
              "rows": [
                [
                  "P99 write latency",
                  "< 5ms",
                  "No data"
                ],
                [
                  "P99 fsync latency",
                  "Measured value",
                  "No data"
                ],
                [
                  "P99 read latency",
                  "Sub-ms (page cache)",
                  "No data"
                ],
                [
                  "Write throughput",
                  "~1000 ops/s",
                  "No data"
                ],
                [
                  "Total rows written",
                  "Non-zero count",
                  "No data"
                ]
              ],
              "caption": "All primary experiment metrics returned empty results due to pod termination before final scrape"
            }
          ]
        },
        {
          "type": "topic",
          "title": "Hypothesis Assessment",
          "blocks": [
            {
              "type": "text",
              "content": "The hypothesis that fsync-per-write on PD-SSD achieves <5ms p99 write latency remains untested. None of the three research questions — fsync cost, read vs. write latency, or sustained throughput — can be answered from this run."
            },
            {
              "type": "row",
              "blocks": [
                {
                  "type": "metric",
                  "key": "write_p99_latency",
                  "size": "small",
                  "insight": "No data — cannot evaluate the <5ms p99 claim"
                },
                {
                  "type": "metric",
                  "key": "fsync_p99_latency",
                  "size": "small",
                  "insight": "No data — the core fsync cost question is unanswered"
                }
              ]
            },
            {
              "type": "row",
              "blocks": [
                {
                  "type": "metric",
                  "key": "write_throughput",
                  "size": "small",
                  "insight": "No data — throughput ceiling under serialized I/O is unknown"
                },
                {
                  "type": "metric",
                  "key": "read_p99_latency",
                  "size": "small",
                  "insight": "No data — page cache read performance is unmeasured"
                }
              ]
            },
            {
              "type": "callout",
              "variant": "finding",
              "title": "Experiment Inconclusive",
              "content": "This run establishes that the experiment infrastructure works (cluster provisioned, workflow succeeded, monitoring stack deployed) but fails at the critical handoff between workload completion and metrics collection. The hypothesis requires a re-run with lifecycle fixes."
            }
          ]
        },
        {
          "type": "topic",
          "title": "Infrastructure & Cost",
          "blocks": [
            {
              "type": "text",
              "content": "The experiment ran a single e2-standard-4 node for ~37 minutes at a total cost of $0.017. The monitoring stack (full kube-prometheus-stack with Grafana, Alertmanager, and Alloy) is significantly oversized for a single-workload benchmark."
            },
            {
              "type": "comparison",
              "items": [
                {
                  "label": "Total Cost",
                  "value": "$0.017",
                  "description": "37-minute run on a single e2-standard-4 node at on-demand pricing"
                },
                {
                  "label": "Node Type",
                  "value": "e2-standard-4",
                  "description": "4 vCPU, 16 GB RAM — likely oversized given the lightweight workload"
                },
                {
                  "label": "Duration",
                  "value": "37m 11s",
                  "description": "Cluster creation to metrics collection"
                }
              ]
            },
            {
              "type": "table",
              "headers": [
                "Component",
                "Purpose",
                "Right-sized?"
              ],
              "rows": [
                [
                  "naive-db StatefulSet",
                  "Target workload under test",
                  "Yes"
                ],
                [
                  "kube-prometheus-stack",
                  "Full monitoring (Prometheus, Grafana, Alertmanager)",
                  "Over-provisioned — Grafana and Alertmanager unnecessary for benchmark"
                ],
                [
                  "Alloy DaemonSet",
                  "Metrics forwarding",
                  "Could replace entire stack for simple remote-write"
                ],
                [
                  "node-exporter DaemonSet",
                  "Host-level metrics",
                  "Useful as cross-check for disk I/O"
                ]
              ],
              "caption": "The monitoring stack deploys 8+ containers for a single-metric workload"
            }
          ]
        },
        {
          "type": "topic",
          "title": "Security Posture",
          "blocks": [
            {
              "type": "text",
              "content": "The benchmark cluster carries a disproportionate attack surface relative to the workload. Six cluster-scoped RBAC roles, webhook configurations, and plaintext secrets are deployed for what is effectively a single-pod write loop."
            },
            {
              "type": "comparison",
              "items": [
                {
                  "label": "ClusterRoles",
                  "value": "6",
                  "description": "Cluster-wide RBAC grants for Alloy, Prometheus, Grafana, operator, kube-state-metrics, and admission"
                },
                {
                  "label": "NetworkPolicies",
                  "value": "0",
                  "description": "No network segmentation — all pods can communicate freely across namespaces"
                },
                {
                  "label": "Secrets",
                  "value": "2",
                  "description": "Alertmanager and Grafana secrets stored as base64 without external secrets management"
                }
              ]
            },
            {
              "type": "callout",
              "variant": "info",
              "title": "Acceptable for Ephemeral Benchmarks",
              "content": "The security gaps (broad RBAC, no NetworkPolicies, plaintext secrets) are typical of Helm-installed monitoring stacks and present low risk in a 37-minute ephemeral cluster. However, if this experiment template is reused for longer-lived or multi-tenant environments, namespace-scoped roles and network policies should be added."
            }
          ]
        },
        {
          "type": "topic",
          "title": "Recommendations for Re-run",
          "blocks": [
            {
              "type": "recommendation",
              "priority": "p0",
              "title": "Fix metrics collection lifecycle",
              "description": "Ensure the naive-db pod remains running and scrapeable (up=1) for at least 2-3 full Prometheus scrape intervals after the loadgen job completes. Add a readiness gate or post-completion sleep to prevent premature pod termination.",
              "effort": "low"
            },
            {
              "type": "recommendation",
              "priority": "p0",
              "title": "Add data-availability assertions to workflow",
              "description": "Before the workflow transitions to 'Succeeded', verify that write_p99_latency, fsync_p99_latency, and total_rows queries return non-empty results. Fail the workflow explicitly if metrics are missing.",
              "effort": "low"
            },
            {
              "type": "recommendation",
              "priority": "p1",
              "title": "Emit summary gauges at workload completion",
              "description": "Have naive-db publish final computed values (e.g., naivedb_summary_p99_write_seconds, naivedb_summary_total_writes) as gauge metrics that persist the last known state, providing a fallback when histogram rate queries return empty for short-lived workloads.",
              "effort": "medium"
            },
            {
              "type": "recommendation",
              "priority": "p2",
              "title": "Right-size the monitoring stack",
              "description": "Replace the full kube-prometheus-stack with a minimal Prometheus instance using remote-write. Drop Grafana, Alertmanager, and admission webhooks from the benchmark template to free ~1.5 GB RAM and reduce attack surface.",
              "effort": "medium"
            },
            {
              "type": "recommendation",
              "priority": "p2",
              "title": "Use node-level disk I/O as cross-check",
              "description": "Add node_disk_write_time_seconds_total and node_disk_writes_completed_total from node-exporter to the metrics queries. These persist independently of the application pod and can confirm writes occurred even if application metrics are lost.",
              "effort": "low"
            }
          ]
        },
        {
          "type": "text",
          "content": "This experiment successfully provisions infrastructure and runs the workload to completion, but fails at the last mile: capturing the data it was designed to measure. The two P0 fixes — extending pod lifetime past the final scrape and adding data-availability assertions — should resolve the issue for the next run. Until then, the fsync-per-write cost on GKE PD-SSD remains an open question."
        }
      ]
    },
    "summary": "Analysis incomplete",
    "generatedAt": "2026-02-18T01:34:43Z",
    "model": "claude-opus-4-6"
  }
}
