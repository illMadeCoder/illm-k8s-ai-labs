{
  "name": "gw-comp-b5jbs",
  "namespace": "experiments",
  "description": "Gateway comparison v2 — NGINX Ingress vs Traefik vs Envoy Gateway with load testing, per-gateway resource profiling, and latency benchmarks",
  "createdAt": "2026-02-13T21:49:11Z",
  "completedAt": "2026-02-13T23:46:49.950029222Z",
  "durationSeconds": 7058.950029222,
  "phase": "Complete",
  "tags": [
    "comparison",
    "networking",
    "gateway"
  ],
  "hypothesis": {
    "claim": "Under load, Envoy Gateway's two-process architecture (separate control plane + Envoy data-plane proxy) will consume more total resources than NGINX Ingress's single-process model, but deliver lower tail latency because Envoy's event-driven C++ proxy handles concurrent connections more efficiently than NGINX's worker-process model",
    "questions": [
      "What is the idle CPU/memory footprint of each gateway controller and data plane?",
      "How do per-gateway resource profiles change from idle to loaded (50 concurrent users)?",
      "What are the p50/p90/p99 latency distributions per gateway under sustained load?",
      "Which gateway achieves the highest throughput ceiling before error rates increase?",
      "How does Envoy Gateway's data-plane proxy (envoy-default-*) contribute to total resource usage?"
    ],
    "focus": [
      "resource efficiency under load",
      "latency percentiles (p50/p90/p99)",
      "throughput and error rates",
      "idle-vs-loaded resource delta"
    ]
  },
  "analyzerConfig": {
    "sections": [
      "abstract",
      "targetAnalysis",
      "performanceAnalysis",
      "metricInsights",
      "finopsAnalysis",
      "secopsAnalysis",
      "body",
      "capabilitiesMatrix",
      "feedback",
      "architectureDiagram"
    ]
  },
  "targets": [
    {
      "name": "app",
      "clusterName": "gw-comp-b5jbs-app",
      "clusterType": "gke",
      "machineType": "e2-standard-4",
      "nodeCount": 1
    },
    {
      "name": "loadgen",
      "clusterName": "gw-comp-b5jbs-loadgen",
      "clusterType": "gke",
      "machineType": "e2-standard-2",
      "nodeCount": 1,
      "components": [
        "Component/k6-gateway-loadtest"
      ]
    }
  ],
  "workflow": {
    "name": "gw-comp-b5jbs-validation",
    "template": "gateway-comparison-validation",
    "phase": "Succeeded",
    "startedAt": "2026-02-13T23:01:00Z",
    "finishedAt": "2026-02-13T23:46:35Z"
  },
  "metrics": {
    "collectedAt": "2026-02-13T23:46:50.136503135Z",
    "source": "target:cadvisor",
    "timeRange": {
      "start": "2026-02-13T21:49:11Z",
      "end": "2026-02-13T23:46:50.136503135Z",
      "duration": "1h57m39.136503135s",
      "stepSeconds": 0
    },
    "queries": {
      "cpu_by_pod": {
        "query": "container_cpu_usage_seconds_total by pod (cadvisor)",
        "type": "instant",
        "unit": "cores",
        "description": "CPU usage by pod (cumulative seconds)",
        "data": [
          {
            "labels": {
              "pod": "ingress-nginx-controller-766749c598-8svfp"
            },
            "timestamp": "2026-02-13T23:46:50.136503135Z",
            "value": 4.071722
          },
          {
            "labels": {
              "pod": "envoy-gateway-56646c568b-w8kkq"
            },
            "timestamp": "2026-02-13T23:46:50.136503135Z",
            "value": 5.726675
          },
          {
            "labels": {
              "pod": "kube-state-metrics-0"
            },
            "timestamp": "2026-02-13T23:46:50.136503135Z",
            "value": 2.4347950000000003
          },
          {
            "labels": {
              "pod": "operator-64d66c8747-zrmmc"
            },
            "timestamp": "2026-02-13T23:46:50.136503135Z",
            "value": 4.25824
          },
          {
            "labels": {
              "pod": "traefik-6865bcdd9c-x5zx7"
            },
            "timestamp": "2026-02-13T23:46:50.136503135Z",
            "value": 2.505602
          }
        ]
      },
      "cpu_total": {
        "query": "sum(container_cpu_usage_seconds_total) (cadvisor)",
        "type": "instant",
        "unit": "cores",
        "description": "Total CPU usage (cumulative seconds)",
        "data": [
          {
            "labels": {
              "scope": "total"
            },
            "timestamp": "2026-02-13T23:46:50.136503135Z",
            "value": 18.997034000000003
          }
        ]
      },
      "memory_by_pod": {
        "query": "container_memory_working_set_bytes by pod (cadvisor)",
        "type": "instant",
        "unit": "bytes",
        "description": "Memory working set by pod",
        "data": [
          {
            "labels": {
              "pod": "operator-64d66c8747-zrmmc"
            },
            "timestamp": "2026-02-13T23:46:50.136503135Z",
            "value": 23736320
          },
          {
            "labels": {
              "pod": "traefik-6865bcdd9c-x5zx7"
            },
            "timestamp": "2026-02-13T23:46:50.136503135Z",
            "value": 29749248
          },
          {
            "labels": {
              "pod": "ingress-nginx-controller-766749c598-8svfp"
            },
            "timestamp": "2026-02-13T23:46:50.136503135Z",
            "value": 30920704
          },
          {
            "labels": {
              "pod": "envoy-gateway-56646c568b-w8kkq"
            },
            "timestamp": "2026-02-13T23:46:50.136503135Z",
            "value": 35590144
          },
          {
            "labels": {
              "pod": "kube-state-metrics-0"
            },
            "timestamp": "2026-02-13T23:46:50.136503135Z",
            "value": 38301696
          }
        ]
      },
      "memory_total": {
        "query": "sum(container_memory_working_set_bytes) (cadvisor)",
        "type": "instant",
        "unit": "bytes",
        "description": "Total memory working set",
        "data": [
          {
            "labels": {
              "scope": "total"
            },
            "timestamp": "2026-02-13T23:46:50.136503135Z",
            "value": 158298112
          }
        ]
      }
    }
  },
  "costEstimate": {
    "totalUSD": 0.07882494199297901,
    "durationHours": 1.9608194525616667,
    "perTarget": {
      "app": 0.052549961328652674,
      "loadgen": 0.026274980664326337
    },
    "note": "Rough estimate based on on-demand GCE pricing; actual cost may differ."
  },
  "analysis": {
    "finopsAnalysis": {
      "overview": "This gateway comparison experiment ran for ~1.96 hours across two GKE clusters (one e2-standard-4 app node, one e2-standard-2 loadgen node) at an estimated total cost of $0.079. The cost is modest for a benchmarking run, but the infrastructure is oversized relative to the workloads deployed — all five observed pods collectively used only ~19 cumulative CPU-seconds and ~151 MiB of memory, a fraction of the 6 vCPUs and 24 GiB RAM provisioned across both nodes.",
      "costDrivers": [
        "Compute node provisioning is the dominant cost: the e2-standard-4 app cluster accounts for $0.0525 (66.7% of total) and the e2-standard-2 loadgen cluster accounts for $0.0263 (33.3%). Node costs accrue whether pods are active or idle, and the ~2-hour experiment duration means most of the spend went to idle node time outside the ~45-minute active workflow window.",
        "Cluster overhead: GKE management fees ($0.10/hr for standard-tier clusters) may apply per cluster and could exceed the raw compute cost for short-lived experiments like this. Two clusters doubles this overhead. The data shows the experiment existed for ~1.96 hours but the workflow only ran from 23:01 to 23:46 (~45 minutes), meaning over 55% of cluster uptime was non-productive."
      ],
      "projection": "Production projection for running all three gateways 24/7 on non-preemptible nodes: An e2-standard-4 (4 vCPU, 16 GiB) costs ~$0.134/hr on-demand in us-central1. A realistic production setup would use dedicated nodes per gateway for isolation — 3x e2-standard-2 ($0.067/hr each) for the gateways plus 1x e2-standard-4 for backend workloads = $0.335/hr total compute. Monthly: $0.335 × 730 hrs = ~$244.55/month in compute alone. Adding GKE standard-tier cluster management ($0.10/hr × 730 = $73/month per cluster, or $73 if consolidated to one cluster), persistent disks, load balancer ($18/month per forwarding rule × 3 gateways = $54), and egress, a consolidated single-cluster production deployment would cost approximately $370–$420/month. A multi-cluster deployment (as tested) would push this to $590–$660/month due to duplicated cluster fees and control planes.",
      "optimizations": [
        "Consolidate to a single GKE Autopilot cluster for benchmarking: Autopilot charges only for pod resource requests, eliminating idle node cost. Given the low resource consumption observed (~19 CPU-seconds total, ~151 MiB memory), Autopilot could reduce compute costs by 60-70% for short-lived experiments.",
        "Use preemptible/spot VMs for the loadgen cluster: Load generation is stateless and fault-tolerant. Spot e2-standard-2 instances cost ~$0.02/hr vs $0.067/hr (70% savings on the loadgen target, saving ~$0.018 per experiment run).",
        "Reduce cluster idle time by tightening the experiment lifecycle: The workflow ran for only 45 minutes of a ~2-hour experiment. Automating cluster creation and teardown to bracket the workflow tightly would cut costs by approximately 50%.",
        "Right-size the app node: The observed gateway pods use 23–36 MiB RAM each. An e2-standard-2 (2 vCPU, 8 GiB) would be sufficient for all three gateways plus backend pods, saving ~$0.067/hr ($49/month in production)."
      ]
    },
    "secopsAnalysis": {
      "overview": "The experiment deploys three ingress/gateway controllers (NGINX Ingress, Traefik, Envoy Gateway) plus supporting infrastructure (kube-state-metrics, an operator pod) across two GKE clusters. No evidence of network policies, RBAC hardening, or resource limits/quotas is present in the experiment data. Running three gateway controllers simultaneously on a shared node increases the attack surface, as a compromise of any one gateway could allow lateral movement to intercept traffic routed through the others.",
      "findings": [
        "No network policies observed: All three gateway controllers and the backend application appear to share a flat network on the app cluster. In production, each gateway's data-plane pod should be isolated via Kubernetes NetworkPolicies restricting ingress/egress to only the required backends and control-plane endpoints. Without these, a compromised gateway pod can reach all other pods in the cluster.",
        "RBAC exposure from multiple privileged controllers: NGINX Ingress Controller, Traefik, and Envoy Gateway each require cluster-scoped RBAC permissions (ClusterRoles) to watch Ingress/Gateway API resources. Running all three simultaneously triples the number of highly-privileged ServiceAccounts with access to cluster-wide API resources. The Envoy Gateway control-plane pod (envoy-gateway-56646c568b-w8kkq) additionally needs permissions to manage xDS configuration for its data-plane proxy. Any one of these ServiceAccount tokens, if leaked, could be used to enumerate or modify routing rules cluster-wide.",
        "Missing Envoy data-plane proxy metrics indicate a visibility gap: The envoy-default-* data-plane pod is not present in cadvisor metrics, which means either it was not deployed, was deployed in a different namespace not being scraped, or metrics collection failed for it. This is a security observability concern — if the primary traffic-handling pod for Envoy Gateway is not being monitored, anomalous resource consumption (indicative of cryptomining, data exfiltration, or DoS) would go undetected.",
        "No resource limits visible in the experiment data: None of the five observed pods show configured CPU/memory limits. Without resource limits, a single pod under attack (e.g., a slowloris or resource exhaustion attack against one gateway) can starve other pods on the same node, including the other gateway controllers. This is especially critical given all gateways share one e2-standard-4 node.",
        "The k6 load test component runs on a separate cluster (loadgen), which is good practice for preventing load-generation interference with the system under test, but the loadgen cluster's e2-standard-2 node likely has the default GKE node configuration with full API access scopes unless explicitly restricted."
      ],
      "supplyChain": "No image provenance, signing, or SBOM metadata is available in the experiment data. The deployed components — NGINX Ingress Controller, Traefik, Envoy Gateway, and kube-state-metrics — are all open-source projects distributed via public container registries (typically Docker Hub, ghcr.io, or registry.k8s.io). Without evidence of image digest pinning (vs. mutable tags), Cosign signature verification, or SBOM attestation, the supply chain posture is unverified. In production, all gateway images should be pulled by digest, verified against Sigstore/Cosign signatures (NGINX Ingress and Envoy Gateway publish signed images; Traefik publishes to Docker Hub with content trust), and scanned for CVEs. The 'operator' pod (operator-64d66c8747-zrmmc) is of particular concern as its provenance and purpose are unclear from the data — it could be the experiment platform's own operator, but without image reference verification, its trust level cannot be assessed."
    },
    "capabilitiesMatrix": {
      "technologies": [
        "NGINX Ingress",
        "Traefik",
        "Envoy Gateway"
      ],
      "categories": [
        {
          "name": "Resource Efficiency",
          "capabilities": [
            {
              "name": "Cumulative CPU usage (full experiment)",
              "values": {
                "NGINX Ingress": "4.07 CPU-sec (moderate)",
                "Traefik": "2.51 CPU-sec (lowest)",
                "Envoy Gateway": "5.73 CPU-sec control-plane only (highest observed, data-plane missing)"
              }
            },
            {
              "name": "Memory working set (snapshot)",
              "values": {
                "NGINX Ingress": "~29.5 MiB (moderate)",
                "Traefik": "~28.4 MiB (lowest)",
                "Envoy Gateway": "~33.9 MiB control-plane only (highest observed, data-plane missing)"
              }
            },
            {
              "name": "Architecture overhead",
              "values": {
                "NGINX Ingress": "Single pod — controller + NGINX in one process",
                "Traefik": "Single pod — single Go binary, lightest footprint",
                "Envoy Gateway": "Two-pod model but envoy-default-* data-plane pod metrics absent; true total resource cost unknown"
              }
            }
          ]
        },
        {
          "name": "Latency Performance",
          "capabilities": [
            {
              "name": "p50/p90/p99 latency under load",
              "values": {
                "NGINX Ingress": "Not measured — k6 results missing from collected data",
                "Traefik": "Not measured — k6 results missing from collected data",
                "Envoy Gateway": "Not measured — k6 results missing from collected data"
              }
            },
            {
              "name": "Expected tail-latency behavior",
              "values": {
                "NGINX Ingress": "Worker-prefork model; moderate tail latency expected under concurrency",
                "Traefik": "Go GC pauses may inflate p99 under sustained load",
                "Envoy Gateway": "C++ event-driven workers should yield lowest p99, but unverified"
              }
            }
          ]
        },
        {
          "name": "Throughput & Reliability",
          "capabilities": [
            {
              "name": "Throughput ceiling (rps)",
              "values": {
                "NGINX Ingress": "Not measured — k6 output not captured",
                "Traefik": "Not measured — k6 output not captured",
                "Envoy Gateway": "Not measured — k6 output not captured"
              }
            },
            {
              "name": "Error rate under load",
              "values": {
                "NGINX Ingress": "Not measured",
                "Traefik": "Not measured",
                "Envoy Gateway": "Not measured"
              }
            }
          ]
        },
        {
          "name": "Observability & Data Completeness",
          "capabilities": [
            {
              "name": "Pod-level resource attribution",
              "values": {
                "NGINX Ingress": "Complete — single pod captured",
                "Traefik": "Complete — single pod captured",
                "Envoy Gateway": "Incomplete — only control-plane pod (envoy-gateway) captured; envoy-default-* data-plane proxy pod missing"
              }
            },
            {
              "name": "Load test results captured",
              "values": {
                "NGINX Ingress": "Missing — k6 summary not in experiment data",
                "Traefik": "Missing — k6 summary not in experiment data",
                "Envoy Gateway": "Missing — k6 summary not in experiment data"
              }
            }
          ]
        }
      ],
      "summary": "No winner can be declared. The experiment's core hypothesis — that Envoy Gateway trades higher total resource consumption for lower tail latency — is untestable because (1) the k6 load-test latency and throughput results were not captured in the experiment output, and (2) the envoy-default-* data-plane proxy pod's resource metrics are missing, making Envoy Gateway's true resource cost unknown. Based solely on the partial cAdvisor data collected, Traefik showed the lightest resource footprint (~2.5 CPU-sec, ~28 MiB), but without latency, throughput, or error-rate data this is an incomplete and potentially misleading comparison."
    },
    "feedback": {
      "recommendations": [
        "Capture and persist k6 load-test output (HTTP request duration percentiles, requests/sec, error counts) as a first-class experiment artifact — this is the single most critical gap preventing any latency or throughput comparison.",
        "Add a cadvisor or Prometheus scrape target for the envoy-default-* data-plane proxy pod so that Envoy Gateway's total resource cost (control-plane + data-plane) can be measured and compared fairly against the single-pod architectures of NGINX Ingress and Traefik.",
        "Collect time-series CPU and memory data (not just cumulative snapshots) to distinguish idle-phase baseline resource usage from loaded-phase resource usage, enabling the idle-to-loaded delta analysis the hypothesis requires.",
        "Run each gateway's load test sequentially with a cool-down period between tests and tag metrics by phase, so per-gateway resource deltas can be isolated rather than blended into a single experiment-wide snapshot."
      ],
      "experimentDesign": [
        "Replace instant-query cadvisor snapshots with range queries (e.g., rate(container_cpu_usage_seconds_total[30s])) at 15-second step intervals to produce time-series data; label each scrape window with the active gateway and load phase (idle, ramp-up, steady-state, cool-down) so resource profiles can be plotted per gateway per phase.",
        "Integrate k6 handleSummary() or --out json to export structured load-test results (p50/p90/p99 latencies, throughput, error rate) directly into the experiment data pipeline, rather than relying on separate log collection.",
        "Add a pre-flight validation step that confirms all expected pods (including envoy-default-*) are present in the metrics scrape targets before the load phase begins, preventing incomplete data collection from going undetected."
      ]
    },
    "body": {
      "blocks": [
        {
          "type": "text",
          "content": "This experiment compared three Kubernetes gateway controllers — NGINX Ingress, Traefik, and Envoy Gateway — across resource efficiency, latency, and throughput dimensions. While resource metrics were captured via cAdvisor, critical gaps in data collection prevent a definitive verdict on the core hypothesis."
        },
        {
          "type": "callout",
          "variant": "warning",
          "title": "Inconclusive Experiment: Key Data Missing",
          "content": "The k6 load-test results (latency percentiles, throughput, error rates) were not captured in the experiment output, and Envoy Gateway's data-plane proxy pod (envoy-default-*) is absent from metrics. The hypothesis — that Envoy trades higher resource cost for lower tail latency — cannot be evaluated with the collected data."
        },
        {
          "type": "topic",
          "title": "Resource Efficiency",
          "blocks": [
            {
              "type": "text",
              "content": "Resource consumption was measured via cumulative cAdvisor snapshots taken at experiment completion. All three gateways showed minimal resource usage, collectively consuming under 20 CPU-seconds across a ~2-hour run."
            },
            {
              "type": "row",
              "blocks": [
                {
                  "type": "metric",
                  "key": "cpu_by_pod",
                  "size": "small",
                  "insight": "Envoy Gateway's control-plane pod alone used 5.73 CPU-sec — 40% more than NGINX and 128% more than Traefik. However, this excludes the missing data-plane proxy."
                },
                {
                  "type": "metric",
                  "key": "memory_by_pod",
                  "size": "small",
                  "insight": "Memory footprints were tightly clustered between 28–34 MiB across gateways, with infrastructure pods (kube-state-metrics) actually consuming more than any gateway."
                }
              ]
            },
            {
              "type": "comparison",
              "items": [
                {
                  "label": "Traefik",
                  "value": "2.51 CPU-sec / 28.4 MiB",
                  "description": "Lightest observed footprint. Single Go binary architecture with no external dependencies."
                },
                {
                  "label": "NGINX Ingress",
                  "value": "4.07 CPU-sec / 29.5 MiB",
                  "description": "Moderate usage. Single pod bundles the controller and NGINX worker processes."
                },
                {
                  "label": "Envoy Gateway",
                  "value": "5.73 CPU-sec / 33.9 MiB",
                  "description": "Highest observed — but this is control-plane only. The envoy-default-* data-plane proxy pod is missing from metrics entirely."
                }
              ]
            },
            {
              "type": "callout",
              "variant": "finding",
              "title": "Envoy Gateway's True Cost Is Unknown",
              "content": "Envoy Gateway uses a two-pod architecture: a control-plane manager and a separate envoy-default-* data-plane proxy that handles actual traffic. The data-plane pod was not captured in cAdvisor metrics — either it wasn't scraped, wasn't deployed in the monitored namespace, or failed to start. Without it, Envoy Gateway's total resource cost cannot be compared to the single-pod architectures of NGINX and Traefik."
            }
          ]
        },
        {
          "type": "topic",
          "title": "Latency & Throughput Performance",
          "blocks": [
            {
              "type": "text",
              "content": "The experiment's hypothesis centered on tail-latency differences under load, but the k6 load-test output was not persisted as an experiment artifact. None of the five core questions about latency and throughput can be answered."
            },
            {
              "type": "table",
              "headers": [
                "Metric",
                "NGINX Ingress",
                "Traefik",
                "Envoy Gateway"
              ],
              "rows": [
                [
                  "p50 latency",
                  "Not captured",
                  "Not captured",
                  "Not captured"
                ],
                [
                  "p90 latency",
                  "Not captured",
                  "Not captured",
                  "Not captured"
                ],
                [
                  "p99 latency",
                  "Not captured",
                  "Not captured",
                  "Not captured"
                ],
                [
                  "Throughput (rps)",
                  "Not captured",
                  "Not captured",
                  "Not captured"
                ],
                [
                  "Error rate",
                  "Not captured",
                  "Not captured",
                  "Not captured"
                ]
              ],
              "caption": "All latency and throughput metrics are missing from experiment data. The k6 workflow succeeded but its output was not collected."
            },
            {
              "type": "text",
              "content": "The workflow phase shows \"Succeeded,\" confirming the k6 tests ran. The results were simply not captured into the experiment data pipeline — a collection gap, not an execution failure."
            },
            {
              "type": "capabilityRow",
              "capability": "Expected tail-latency behavior",
              "values": {
                "NGINX Ingress": "Worker-prefork model; moderate tail latency expected under concurrency",
                "Traefik": "Go GC pauses may inflate p99 under sustained load",
                "Envoy Gateway": "C++ event-driven workers should yield lowest p99 — but unverified"
              }
            }
          ]
        },
        {
          "type": "topic",
          "title": "Data Completeness & Observability",
          "blocks": [
            {
              "type": "text",
              "content": "The experiment collected only instant-query cAdvisor snapshots at completion time, providing a single cumulative data point per pod rather than time-series data. This makes it impossible to distinguish idle-phase from loaded-phase resource behavior."
            },
            {
              "type": "table",
              "headers": [
                "Data Gap",
                "Impact",
                "Severity"
              ],
              "rows": [
                [
                  "k6 load-test output not captured",
                  "Cannot evaluate latency, throughput, or error rates — the experiment's primary objectives",
                  "Critical"
                ],
                [
                  "envoy-default-* pod missing from metrics",
                  "Envoy Gateway's total resource cost unknown; comparison is unfair",
                  "Critical"
                ],
                [
                  "Instant queries instead of range queries",
                  "Cannot plot resource usage over time or isolate per-phase behavior",
                  "High"
                ],
                [
                  "No idle vs. loaded phase tagging",
                  "Cannot compute resource deltas under load — a core hypothesis question",
                  "High"
                ],
                [
                  "No resource limits configured",
                  "Cannot assess production readiness or noisy-neighbor risk",
                  "Medium"
                ]
              ],
              "caption": "Five data gaps prevent meaningful conclusions from this experiment run."
            },
            {
              "type": "metric",
              "key": "cpu_total",
              "size": "large",
              "insight": "Total cumulative CPU across all 5 pods was just 19 CPU-seconds over ~2 hours — the e2-standard-4 node (4 vCPUs) was dramatically underutilized. The workloads consumed less than 0.14% of available CPU capacity."
            },
            {
              "type": "metric",
              "key": "memory_total",
              "size": "large",
              "insight": "Total memory working set of ~151 MiB represents less than 1% of the 16 GiB provisioned on the app node."
            }
          ]
        },
        {
          "type": "topic",
          "title": "Cost Analysis & Infrastructure Sizing",
          "blocks": [
            {
              "type": "text",
              "content": "The experiment ran for ~1.96 hours across two GKE clusters at an estimated cost of $0.079. The infrastructure was significantly oversized for the deployed workloads, and over half of the cluster uptime was non-productive."
            },
            {
              "type": "comparison",
              "items": [
                {
                  "label": "App Cluster",
                  "value": "$0.053 (66.7%)",
                  "description": "e2-standard-4 node (4 vCPU, 16 GiB). Hosted all three gateways plus infrastructure pods."
                },
                {
                  "label": "Loadgen Cluster",
                  "value": "$0.026 (33.3%)",
                  "description": "e2-standard-2 node (2 vCPU, 8 GiB). Ran the k6 load test in isolation."
                }
              ]
            },
            {
              "type": "row",
              "blocks": [
                {
                  "type": "callout",
                  "variant": "info",
                  "title": "Active vs. Idle Time",
                  "content": "The workflow ran from 23:01 to 23:46 (~45 min) within a ~2-hour experiment window. Over 55% of cluster uptime was non-productive idle time accruing compute costs."
                },
                {
                  "type": "callout",
                  "variant": "info",
                  "title": "Production Projection",
                  "content": "Running all three gateways 24/7 on a single consolidated cluster: ~$370–420/month. A multi-cluster deployment (as tested) would cost $590–660/month due to duplicated control planes."
                }
              ]
            },
            {
              "type": "recommendation",
              "priority": "p1",
              "title": "Switch to GKE Autopilot for benchmarking experiments",
              "description": "Autopilot charges only for pod resource requests, eliminating idle node costs. Given the observed resource consumption (~19 CPU-seconds, ~151 MiB), Autopilot could reduce compute costs by 60-70%.",
              "effort": "medium"
            },
            {
              "type": "recommendation",
              "priority": "p1",
              "title": "Right-size the app node to e2-standard-2",
              "description": "Gateway pods used 23–36 MiB RAM each. An e2-standard-2 (2 vCPU, 8 GiB) is sufficient for all three gateways and would save ~$49/month in production deployments.",
              "effort": "low"
            }
          ]
        },
        {
          "type": "topic",
          "title": "Security Posture",
          "blocks": [
            {
              "type": "text",
              "content": "Running three gateway controllers simultaneously on a shared node with no observed network policies, resource limits, or RBAC hardening creates an expanded attack surface. Each controller requires cluster-scoped permissions that, if compromised, could affect all routing rules."
            },
            {
              "type": "table",
              "headers": [
                "Concern",
                "Status",
                "Risk"
              ],
              "rows": [
                [
                  "Network policies",
                  "Not configured",
                  "A compromised gateway pod can reach all other pods on the flat network"
                ],
                [
                  "Resource limits",
                  "Not configured",
                  "A resource-exhaustion attack on one gateway can starve the others"
                ],
                [
                  "RBAC scope",
                  "3x cluster-scoped ServiceAccounts",
                  "Tripled attack surface for credential theft; any leaked token can modify routing cluster-wide"
                ],
                [
                  "Image provenance",
                  "No digest pinning or signature verification observed",
                  "Supply chain integrity unverified for all deployed components"
                ],
                [
                  "Envoy data-plane monitoring",
                  "Pod absent from metrics",
                  "Anomalous behavior in the traffic-handling proxy would go undetected"
                ]
              ],
              "caption": "Security findings based on observable experiment configuration."
            },
            {
              "type": "recommendation",
              "priority": "p2",
              "title": "Add NetworkPolicies isolating each gateway's data plane",
              "description": "Restrict ingress/egress on each gateway pod to only required backends and control-plane endpoints. Prevents lateral movement between gateway controllers on the shared node.",
              "effort": "medium"
            },
            {
              "type": "recommendation",
              "priority": "p2",
              "title": "Configure resource limits on all gateway pods",
              "description": "Without CPU/memory limits, a single pod under attack can consume all node resources. Set limits based on observed baseline usage plus headroom (e.g., 200m CPU, 128Mi memory).",
              "effort": "low"
            }
          ]
        },
        {
          "type": "text",
          "content": "This experiment established the infrastructure and workflow mechanics for gateway comparison but failed to capture the data needed to evaluate its hypothesis. The resource metrics that were collected suggest Traefik has the lightest footprint, but this comparison is incomplete and potentially misleading without Envoy's data-plane metrics or any latency data."
        },
        {
          "type": "callout",
          "variant": "finding",
          "title": "Verdict: Re-run Required",
          "content": "No winner can be declared. The two most critical fixes before re-running are: (1) integrate k6 handleSummary() to export structured latency/throughput results into the experiment data pipeline, and (2) add a pre-flight validation step confirming all expected pods — including envoy-default-* — appear in metrics scrape targets before load testing begins."
        },
        {
          "type": "recommendation",
          "priority": "p0",
          "title": "Capture k6 load-test output as a first-class experiment artifact",
          "description": "Use k6's handleSummary() or --out json to export structured results (p50/p90/p99 latencies, requests/sec, error counts) directly into the experiment data pipeline. This is the single most critical gap preventing any performance comparison.",
          "effort": "low"
        },
        {
          "type": "recommendation",
          "priority": "p0",
          "title": "Add metrics scraping for the Envoy data-plane proxy pod",
          "description": "Ensure the envoy-default-* pod is included in cAdvisor or Prometheus scrape targets. Add a pre-flight check confirming its presence before load testing begins. Without this, Envoy Gateway's total resource cost remains unknown.",
          "effort": "low"
        },
        {
          "type": "recommendation",
          "priority": "p1",
          "title": "Replace instant queries with time-series range queries",
          "description": "Use rate(container_cpu_usage_seconds_total[30s]) at 15-second intervals, tagged by active gateway and load phase (idle, ramp-up, steady-state, cool-down). This enables per-gateway resource profiling and idle-to-loaded delta analysis.",
          "effort": "medium"
        }
      ]
    },
    "summary": "Analysis incomplete",
    "generatedAt": "2026-02-14T00:00:44Z",
    "model": "claude-opus-4-6"
  }
}
