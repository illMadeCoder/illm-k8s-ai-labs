# illm-k8s-ai-lab Roadmap (Final Structure)

**Updated:** 2026-01-17
**Structure:** 10 core phases + 18 appendices + AI-powered tech discovery

---

## Philosophy

**Component Isolation â†’ System Composition â†’ AI Evolution**

1. **Phases 1-9:** Deploy each component + Measure in isolation + FinOps cost analysis
2. **Phase 10:** Measure how components compose as a system + Cost per transaction end-to-end
3. **AI Discovery:** Web scraping to find emerging tech + Automated lab evolution

**Every phase answers:**
- âœ… How do I deploy this component?
- âœ… How do I measure its performance?
- âœ… What does it cost?
- âœ… How does it compare to alternatives?

---

## Core Learning Path (10 Phases)

| # | Phase | What You Build | What You Measure | FinOps Integration |
|---|-------|----------------|------------------|-------------------|
| **1** | Platform Bootstrap & GitOps | Hub, ArgoCD, Crossplane, OpenBao, Argo Workflows | Platform uptime, ArgoCD sync time | Platform running costs |
| **2** | CI/CD & Supply Chain | GitHub Actions, Cosign, SBOM, Kyverno, Image Updater | Build time, image size, scan duration | Build minutes, registry storage |
| **3** | Observability | Prometheus vs VictoriaMetrics, Loki vs ELK, Tempo vs Jaeger, SeaweedFS | Metrics cardinality, log volume, trace sampling | **Cost per metric, cost per GB logs, cost per trace** |
| **4** | Traffic Management | Gateway API, nginx vs Traefik vs Envoy comparison | Requests/sec, latency (p50/p95/p99), connection overhead | **Cost per request, ingress bandwidth cost** |
| **5** | Data & Persistence | PostgreSQL (CloudNativePG), Redis, backup/DR, **database benchmark** | Transactions/sec, query latency, IOPS | **Cost per transaction, cost per GB stored** |
| **6** | Security & Policy | TLS (cert-manager), secrets (ESO+OpenBao), RBAC, Kyverno, NetworkPolicy | Policy evaluation time, TLS handshake overhead | **Security tooling costs, compliance overhead** |
| **7** | Service Mesh | Istio vs Linkerd vs Cilium + **mesh overhead benchmark** | Sidecar latency overhead, control plane CPU/memory | **Mesh overhead cost (sidecar tax)** |
| **8** | Messaging & Events | Kafka vs RabbitMQ vs NATS + **messaging benchmark** | Messages/sec, end-to-end latency, fan-out performance | **Cost per million messages, retention storage cost** |
| **9** | Autoscaling & Resources | HPA, KEDA, VPA, cluster autoscaling | Scale-up time, resource efficiency, cost reduction | **Cost optimization via autoscaling** |
| **10** | **Performance & Cost Engineering** | **Runtime comparison + Full stack composition** | **System-level p99 latency, cost per transaction** | **Cost-efficiency as first-class metric** |

---

## Phase 10: The Grand Finale ğŸ†

**Goal:** Synthesize everything into data-driven system engineering

### 10.1 Runtime Performance Comparison
- Build identical API in: Go, Rust, .NET, Node.js, Bun
- Endpoints: /health, /json, /compute, /database
- Measure: RPS, latency distribution, memory, image size, cold start
- **Cost per million requests by runtime**

### 10.2 Full Stack Composition Benchmark
```
Client â†’ Gateway â†’ Service Mesh â†’ App â†’ Database
         â†“           â†“              â†“       â†“
      Measure    Measure        Measure Measure
```
- Deploy full stack: Runtime + Gateway (nginx/Envoy) + Mesh (Istio/Linkerd) + Database (PostgreSQL)
- Measure p99 latency through entire stack
- Isolate overhead: Baseline vs +Gateway vs +Mesh vs +Observability
- Answer: "What does each layer cost us in latency and $?"

### 10.3 System Trade-Off Analysis
- Performance vs Cost: "The mesh adds 5ms but costs $200/month - worth it?"
- Complexity vs Benefit: "3 layers of observability - which do we actually need?"
- Data-driven decision framework

### 10.4 Cost-Efficiency Dashboard
- Cost per transaction trending
- Cost breakdown by component
- Anomaly detection for cost spikes
- TCO comparison: Self-managed vs cloud-managed

**Portfolio Output:**
- Blog series: "I benchmarked 5 runtimes in Kubernetes"
- Interview material: "Here's how I reduced cost per transaction by 40%"
- GitHub showcase: Data-driven engineering

---

## AI-Powered Tech Discovery (Post Phase 10)

**Goal:** Keep the lab current with ecosystem evolution

### Components
1. **Web Scraping Jobs** (Argo Workflows)
   - Monitor CNCF landscape
   - Track GitHub trending (Kubernetes, Observability, etc.)
   - Parse tech blogs (The New Stack, KubeCon talks)
   - Reddit/HN for emerging patterns

2. **Technology Analysis**
   - Categorize: New component vs improvement vs noise
   - Assess: GitHub stars, contributor activity, production usage
   - Priority: P0 (disruptive), P1 (interesting), P2 (watch)

3. **Automated Suggestions**
   - "Cilium Tetragon is gaining traction - consider adding to Phase 6"
   - "Grafana Beyla (eBPF observability) - potential Phase 3 addition"
   - "Vector (log processor) now has 10k stars - compare vs Promtail?"

4. **Lab Evolution**
   - Generate experiment templates for new tech
   - Propose comparison scenarios
   - Suggest where to integrate in phases

**Implementation:**
- `experiments/ai-discovery/workflows/` - Argo Workflow CronJobs
- `experiments/ai-discovery/scrapers/` - Python scrapers with Beautiful Soup
- `experiments/ai-discovery/analysis/` - Claude integration for categorization
- `experiments/ai-discovery/suggestions/` - Markdown reports with recommendations

---

## Dependency Flow

```
Phase 1 (Platform)
   â””â”€ ArgoCD, Crossplane, OpenBao, Workflows
         â†“
Phase 2 (CI/CD)
   â””â”€ GitHub Actions, Cosign, SBOM, Kyverno
         â†“
Phase 3 (Observability) â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â””â”€ Prometheus, Loki, Tempo, Grafana          â”‚
         â†“                                       â”‚
Phase 4 (Traffic Management)                     â”‚
   â””â”€ Gateway API, nginx/Traefik/Envoy          â”‚
         â†“                                       â”‚
Phase 5 (Data & Persistence)                     â”‚
   â””â”€ PostgreSQL, Redis, backups                â”‚
         â†“                                       â”‚
Phase 6 (Security & Policy) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â””â”€ TLS, secrets, RBAC, NetworkPolicy
         â†“
Phase 7 (Service Mesh)
   â””â”€ Istio, Linkerd, Cilium
         â†“
Phase 8 (Messaging & Events)
   â””â”€ Kafka, RabbitMQ, NATS
         â†“
Phase 9 (Autoscaling & Resources)
   â””â”€ HPA, KEDA, VPA, cluster autoscaling
         â†“
Phase 10 (Performance & Cost Engineering) â† THE CAPSTONE
   â”œâ”€ Runtime comparison (Go/Rust/.NET/Node/Bun)
   â”œâ”€ Full stack composition: Runtimeâ†’Gatewayâ†’Meshâ†’Appâ†’DB
   â”œâ”€ Cost per transaction end-to-end
   â””â”€ System trade-off documentation
         â†“
AI-Powered Tech Discovery â† CONTINUOUS EVOLUTION
   â””â”€ Web scraping â†’ Analysis â†’ Suggestions â†’ Lab updates
```

---

## Advanced Topics (18 Appendices)

**Optional deep dives after core phases:**

### Cloud & Platform Engineering
- **A:** MLOps & AI Infrastructure
- **G:** Deployment Strategies (rolling, blue-green, canary, feature flags)
- **P:** Chaos Engineering
- **Q:** Advanced Workflow Patterns
- **R:** Internal Developer Platforms (Backstage)

### Security & Compliance
- **B:** Identity & Authentication
- **C:** PKI & Certificate Management
- **D:** Compliance & Security Operations
- **O:** SLSA Framework Deep Dive

### Architecture & Design
- **E:** Distributed Systems Fundamentals
- **F:** API Design & Contracts
- **H:** gRPC & HTTP/2 Patterns
- **K:** Event-Driven Architecture

### Performance & Operations
- **I:** Container & Runtime Internals
- **J:** Performance Engineering
- **L:** Database Internals
- **M:** SRE Practices & Incident Management
- **S:** Web Serving Internals

### Multi-Cloud
- **N:** Multi-Cloud & Disaster Recovery

---

## Current Status

**Phase 3: Observability - 60% Complete**

Validated:
- âœ… Prometheus + Grafana (metrics-app, RED dashboards)
- âœ… Victoria Metrics comparison
- âœ… SeaweedFS object storage

Backlog (needs validation):
- [ ] Loki tutorial + cost per GB logs
- [ ] Elasticsearch tutorial
- [ ] Logging comparison (Loki vs ELK)
- [ ] OpenTelemetry tutorial + cost per trace
- [ ] Tempo tutorial
- [ ] Jaeger tutorial
- [ ] Tracing comparison (Tempo vs Jaeger)
- [ ] Pyrra SLOs
- [ ] Observability cost management (cardinality, retention)

**Next:** Validate all 9 backlog experiments (2 weeks)

---

## Timeline to Portfolio-Ready

| Milestone | Duration | Cumulative |
|-----------|----------|------------|
| Phase 3 validation | 2 weeks | 2 weeks |
| Roadmap restructure | 1 week | 3 weeks |
| Phase 4 (Traffic Management) | 3-4 weeks | 6-7 weeks |
| Phase 5 (Data & Persistence) | 3-4 weeks | 9-11 weeks |
| Phase 6 (Security & Policy) | 4-5 weeks | 13-16 weeks |
| Phase 7 (Service Mesh) | 3-4 weeks | 16-20 weeks |
| Phase 8 (Messaging & Events) | 3-4 weeks | 19-24 weeks |
| Phase 9 (Autoscaling) | 2-3 weeks | 21-27 weeks |
| Phase 10 (Grand Finale) | 3-4 weeks | 24-31 weeks |
| AI Tech Discovery | 2-3 weeks | 26-34 weeks |

**Total:** ~5-8 months to complete (realistically 6 months)

---

## Success Metrics

### Current State
- âœ… 2 phases complete (Platform, CI/CD)
- ğŸš§ 1 phase in progress (Observability ~60%)
- ğŸ“ 13 ADRs documented
- ğŸ—ï¸ 8 experiments validated

### Target State (6 months)
- âœ… 10 core phases complete
- ğŸ“ 40+ ADRs documenting decisions
- ğŸ—ï¸ 50-55 experiments validated
- ğŸ¯ Phase 10 capstone: Runtime comparison + full stack benchmark
- ğŸ¤– AI tech discovery running continuously
- ğŸ’¼ Portfolio-ready: Blog posts, GitHub showcase, interview material

---

## What Changed (Consolidation)

**Before:** 16 phases, ~80-90 experiments, 10-12 months
**After:** 10 phases, ~50-55 experiments, 5-6 months

### Changes
- âœ… **Kept Phase 15** - Elevated to Phase 10 (the capstone)
- âœ… **FinOps integrated** - Every phase now includes cost measurements
- âœ… **Benchmarks preserved** - Database (Phase 5), Messaging (Phase 8), Mesh (Phase 7), Runtime (Phase 10)
- âœ… **Security consolidated** - Phase 7 + 8 â†’ Phase 6
- â¬‡ï¸ **Moved to appendices:** Deployment strategies, Chaos, gRPC deep dive, Advanced workflows, Backstage, Web serving details

### Why This Works
1. **Component isolation** (Phases 3-9) teaches measurement expertise
2. **System composition** (Phase 10) teaches full-stack optimization
3. **FinOps first-class** demonstrates cost-conscious engineering
4. **AI discovery** demonstrates forward-thinking architecture

---

## Quick Start

```bash
# Prerequisites: Docker, kubectl, task, helm

task hub:bootstrap                      # Create hub cluster
task hub:conduct -- prometheus-tutorial # Run an experiment
task hub:down -- prometheus-tutorial    # Cleanup
task hub:destroy                        # Destroy cluster
```

---

## Documents

- [Strategic Review](strategic-review-2026-01.md) - Initial assessment
- [Consolidation Analysis](roadmap-consolidation-analysis.md) - Detailed phase analysis
- [Consolidation Summary](roadmap-consolidation-summary.md) - Visual before/after
- [GitOps Patterns](gitops-patterns.md)
- [ADRs](adrs/)
